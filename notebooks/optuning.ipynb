{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aadaf09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47604d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(experiment_name):\n",
    "  \"\"\"\n",
    "  Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.\n",
    "\n",
    "  This function checks if an experiment with the given name exists within MLflow.\n",
    "  If it does, the function returns its ID. If not, it creates a new experiment\n",
    "  with the provided name and returns its ID.\n",
    "\n",
    "  Parameters:\n",
    "  - experiment_name (str): Name of the MLflow experiment.\n",
    "\n",
    "  Returns:\n",
    "  - str: ID of the existing or newly created MLflow experiment.\n",
    "  \"\"\"\n",
    "\n",
    "  if experiment := mlflow.get_experiment_by_name(experiment_name):\n",
    "      return experiment.experiment_id\n",
    "  else:\n",
    "      return mlflow.create_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4080f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = get_or_create_experiment(\"XGBoost_Duration_3moves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb91259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "def get_train_split(dp):\n",
    "    x = dp.drop(columns=\"winner\")\n",
    "    y = dp.get(\"winner\")\n",
    "    return train_test_split(x, y, random_state=42)\n",
    "\n",
    "\n",
    "def load_raw_dataset(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the CSV dataset from disk.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "def filter_to_rated(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Keep only rated games (rated == True).\"\"\"\n",
    "    if 'rated' not in df.columns:\n",
    "        return df.copy()\n",
    "    return df[df['rated'] == True].copy()\n",
    "\n",
    "\n",
    "def remove_duplicate_ids(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicate rows sharing the same game `id` (keep first).\"\"\"\n",
    "    if 'id' not in df.columns:\n",
    "        return df.copy()\n",
    "    return df.drop_duplicates(subset='id', keep='first').copy()\n",
    "\n",
    "\n",
    "def remove_duplicate_games(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicates that share the same (created_at, white_id, black_id).\"\"\"\n",
    "    required = {'created_at', 'white_id', 'black_id'}\n",
    "    if not required.issubset(df.columns):\n",
    "        return df.copy()\n",
    "    return df.drop_duplicates(subset=['created_at', 'white_id', 'black_id'], keep='first').copy()\n",
    "\n",
    "\n",
    "def add_game_duration_seconds(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add `time` as seconds between `last_move_at` and `created_at`, then drop both timestamp columns.\"\"\"\n",
    "    cols = {'created_at', 'last_move_at'}\n",
    "    out = df.copy()\n",
    "    if cols.issubset(out.columns):\n",
    "        out['time'] = (\n",
    "                pd.to_datetime(out['last_move_at'], unit='ms') - pd.to_datetime(out['created_at'], unit='ms')\n",
    "        ).dt.total_seconds()\n",
    "        out = out.drop(columns=['last_move_at', 'created_at'])\n",
    "    return out\n",
    "\n",
    "\n",
    "def split_by_duration_variants(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Create two variants:\n",
    "    - dataset_duration: keep only rows where time != 0 and time != 10000.0\n",
    "    - dataset_noduration: drop the `time` column entirely (keep all rows)\n",
    "    If `time` is missing, returns (df copy, df copy without `time`).\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    if 'time' in out.columns:\n",
    "        dataset_duration = out[(out['time'] != 0) & (out['time'] != 10000.0)].copy()\n",
    "        dataset_noduration = out.drop(columns=['time']).copy()\n",
    "    else:\n",
    "        dataset_duration = out.copy()\n",
    "        dataset_noduration = out.copy()\n",
    "    return dataset_duration, dataset_noduration\n",
    "\n",
    "\n",
    "def drop_columns(df: pd.DataFrame, columns: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"Drop provided columns if present. No-op when `columns` is None or empty.\"\"\"\n",
    "    if not columns:\n",
    "        return df.copy()\n",
    "    return df.drop(columns=columns, errors='ignore').copy()\n",
    "\n",
    "\n",
    "def first_k(s: str, fk: int) -> Optional[str]:\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    parts = s.split()\n",
    "    if len(parts) == 0:\n",
    "        return None\n",
    "    return ' '.join(parts[:fk]) if len(parts) >= fk else s\n",
    "\n",
    "\n",
    "def keep_first_n_moves(\n",
    "        df: pd.DataFrame,\n",
    "        n: int,\n",
    "        only_n: bool = True,\n",
    "        new_column: Optional[str] = None,\n",
    "        add_all_prefix: str = 'moves_'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create truncated move sequences.\n",
    "\n",
    "    - If only_n is True: keep exactly the first n SAN tokens. If `new_column` is None,\n",
    "      overwrite `moves`; otherwise, write to `new_column`.\n",
    "    - If only_n is False: add cumulative columns for k in [1..n] named\n",
    "      f\"{add_all_prefix}{k}\", each containing the first k moves. The original\n",
    "      `moves` column is not preserved.\n",
    "    \"\"\"\n",
    "    if 'moves' not in df.columns:\n",
    "        return df.copy()\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    if only_n:\n",
    "        col = 'moves' if new_column is None else new_column\n",
    "        out[col] = out['moves'].apply(lambda m: first_k(m, n))\n",
    "    else:\n",
    "        for k in range(1, max(1, n) + 1):\n",
    "            out[f\"{add_all_prefix}{k}\"] = out['moves'].apply(lambda m: first_k(m, k))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def clean_chess_data(\n",
    "        file_path: str,\n",
    "        columns_to_drop: Optional[List[str]] = None,\n",
    "        moves_n: Optional[int] = None,\n",
    "        moves_only_n: bool = True,\n",
    "        moves_new_column: Optional[str] = None,\n",
    "        moves_add_all_prefix: str = 'moves_'\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"High-level pipeline that reproduces the notebook cleaning steps.\n",
    "\n",
    "    Steps:\n",
    "    1) load -> 2) filter rated -> 3) remove duplicate ids ->\n",
    "    4) remove duplicate games by (created_at, white_id, black_id) ->\n",
    "    5) add `time` (seconds) and drop raw timestamps ->\n",
    "    6) create two variants: (duration != 0 AND != 10000.0) and (no `time`) ->\n",
    "    7) optionally derive first-n moves (single or cumulative) ->\n",
    "    8) drop requested columns in both variants.\n",
    "\n",
    "    Returns a dict with keys: 'duration', 'noduration'.\n",
    "    \"\"\"\n",
    "    df = load_raw_dataset(file_path)\n",
    "    df = filter_to_rated(df)\n",
    "    df = remove_duplicate_ids(df)\n",
    "    df = remove_duplicate_games(df)\n",
    "    df = add_game_duration_seconds(df)\n",
    "\n",
    "    dataset_duration, dataset_noduration = split_by_duration_variants(df)\n",
    "\n",
    "    # Optionally derive first-n moves on both variants before dropping columns\n",
    "    if moves_n is not None and moves_n > 0:\n",
    "        dataset_duration = keep_first_n_moves(\n",
    "            dataset_duration,\n",
    "            n=moves_n,\n",
    "            only_n=moves_only_n,\n",
    "            new_column=moves_new_column,\n",
    "            add_all_prefix=moves_add_all_prefix,\n",
    "        )\n",
    "        dataset_noduration = keep_first_n_moves(\n",
    "            dataset_noduration,\n",
    "            n=moves_n,\n",
    "            only_n=moves_only_n,\n",
    "            new_column=moves_new_column,\n",
    "            add_all_prefix=moves_add_all_prefix,\n",
    "        )\n",
    "\n",
    "    # Default columns to drop based on the notebook\n",
    "    default_drop = ['id', 'white_id','black_id','opening_name', 'moves']\n",
    "    cols = columns_to_drop if columns_to_drop is not None else default_drop\n",
    "\n",
    "    dataset_duration = drop_columns(dataset_duration, cols)\n",
    "    dataset_noduration = drop_columns(dataset_noduration, cols)\n",
    "\n",
    "    return {\n",
    "        'duration': dataset_duration,\n",
    "        'noduration': dataset_noduration,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7947219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8049 entries, 9291 to 20057\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   turns           8049 non-null   int64  \n",
      " 1   victory_status  8049 non-null   object \n",
      " 2   winner          8049 non-null   object \n",
      " 3   increment_code  8049 non-null   object \n",
      " 4   white_rating    8049 non-null   int64  \n",
      " 5   black_rating    8049 non-null   int64  \n",
      " 6   opening_eco     8049 non-null   object \n",
      " 7   opening_ply     8049 non-null   int64  \n",
      " 8   time            8049 non-null   float64\n",
      " 9   moves_1         8049 non-null   object \n",
      " 10  moves_2         8049 non-null   object \n",
      " 11  moves_3         8049 non-null   object \n",
      "dtypes: float64(1), int64(4), object(7)\n",
      "memory usage: 817.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df_3moves = clean_chess_data(\n",
    "    \"../res/games.csv\",\n",
    "    moves_n=3,\n",
    "    moves_only_n=False\n",
    ")\n",
    "\n",
    "df_duration, df_noduration = df_3moves[\"duration\"], df_3moves[\"noduration\"]\n",
    "\n",
    "df_duration = df_duration.drop(columns=[\"rated\"])\n",
    "df_duration.head()\n",
    "df_duration.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49404f13",
   "metadata": {},
   "source": [
    "We need to convert the object columns:\n",
    "```\n",
    " 0   rated           8049 non-null   bool   \n",
    " 1   turns           8049 non-null   int64  \n",
    " 2   victory_status  8049 non-null   object \n",
    " 3   winner          8049 non-null   object \n",
    " 4   increment_code  8049 non-null   object \n",
    " 5   white_rating    8049 non-null   int64  \n",
    " 6   black_rating    8049 non-null   int64  \n",
    " 7   opening_eco     8049 non-null   object \n",
    " 8   opening_ply     8049 non-null   int64  \n",
    " 9   time            8049 non-null   float64\n",
    " 10  moves_1         8049 non-null   object \n",
    " 11  moves_2         8049 non-null   object \n",
    " 12  moves_3         8049 non-null   object \n",
    " ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "239d6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "mlflow.set_experiment(experiment_id=experiment_id)\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "# Ensure target exists\n",
    "assert \"winner\" in df_duration.columns, f\"winner not in: {df_duration.columns.tolist()}\"\n",
    "\n",
    "# Define features/target\n",
    "X = df_duration.drop(columns=[\"winner\"])\n",
    "y = df_duration[\"winner\"].copy()\n",
    "\n",
    "# Split\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Encode y\n",
    "le = LabelEncoder()\n",
    "train_y_enc = le.fit_transform(train_y)\n",
    "valid_y_enc = le.transform(valid_y)\n",
    "\n",
    "# Build preprocessing strictly from X (no 'winner' here)\n",
    "cat_cols = X.select_dtypes(['object']).columns\n",
    "num_cols = X.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)  # sklearn >=1.2\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)         # sklearn <1.2\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', ohe, cat_cols),\n",
    "    ('num', 'passthrough', num_cols),\n",
    "])\n",
    "\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', XGBClassifier(objective='multi:softprob', num_class=len(le.classes_))),\n",
    "])\n",
    "\n",
    "# Fit\n",
    "model.fit(train_x, train_y_enc)\n",
    "\n",
    "# Optional: encoded features (pandas DataFrame)\n",
    "X_train_enc = model[:-1].transform(train_x)\n",
    "X_valid_enc = model[:-1].transform(valid_x)\n",
    "\n",
    "# Predict (decoded back to original labels)\n",
    "y_pred_enc = model.predict(valid_x)\n",
    "y_pred = le.inverse_transform(y_pred_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a42242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7888198757763976\n",
      "[[ 801    1  264]\n",
      " [   2  100    0]\n",
      " [ 242    1 1004]]\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(valid_y, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(valid_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde4064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iaesiee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
